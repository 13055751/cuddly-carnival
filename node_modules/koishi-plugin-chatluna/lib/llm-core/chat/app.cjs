var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __name = (target, value) => __defProp(target, "name", { value, configurable: true });
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);

// src/llm-core/chat/app.ts
var app_exports = {};
__export(app_exports, {
  ChatInterface: () => ChatInterface
});
module.exports = __toCommonJS(app_exports);
var import_count_tokens = require("koishi-plugin-chatluna/llm-core/utils/count_tokens");
var import_langchain = require("koishi-plugin-chatluna/llm-core/memory/langchain");
var import_koishi_plugin_chatluna = require("koishi-plugin-chatluna");
var import_error = require("koishi-plugin-chatluna/utils/error");
var import_message = require("koishi-plugin-chatluna/llm-core/memory/message");
var import_in_memory = require("koishi-plugin-chatluna/llm-core/model/in_memory");
var import_client = require("koishi-plugin-chatluna/llm-core/platform/client");
var import_model = require("koishi-plugin-chatluna/llm-core/platform/model");
var import_messages = require("@langchain/core/messages");
var import_string = require("koishi-plugin-chatluna/utils/string");
var ChatInterface = class {
  constructor(ctx, input) {
    this.ctx = ctx;
    this._input = input;
  }
  static {
    __name(this, "ChatInterface");
  }
  _input;
  _chatHistory;
  _chains = {};
  _embeddings;
  _errorCountsMap = {};
  _chatCount = 0;
  async handleChatError(error, config) {
    const configMD5 = config.md5();
    if (error instanceof import_error.ChatLunaError && error.errorCode === import_error.ChatLunaErrorCode.API_UNSAFE_CONTENT) {
      throw error;
    }
    this._errorCountsMap[configMD5] = this._errorCountsMap[configMD5] ?? [];
    const errorTimes = this._errorCountsMap[configMD5];
    errorTimes.push(Date.now());
    if (errorTimes.length > config.value.maxRetries * 3) {
      this._errorCountsMap[configMD5] = errorTimes.slice(
        -config.value.maxRetries * 3
      );
    }
    const recentErrors = errorTimes.slice(-config.value.maxRetries);
    if (recentErrors.length >= config.value.maxRetries && checkRange(recentErrors, 1e3 * 60 * 20)) {
      await this.disableConfig(config);
    }
    if (error instanceof import_error.ChatLunaError) {
      throw error;
    }
    throw new import_error.ChatLunaError(import_error.ChatLunaErrorCode.UNKNOWN_ERROR, error);
  }
  async disableConfig(config) {
    const configMD5 = config.md5();
    delete this._chains[configMD5];
    delete this._errorCountsMap[configMD5];
    const service = this.ctx.chatluna.platform;
    await service.makeConfigStatus(config.value, false);
  }
  async chat(arg) {
    const [wrapper, config] = await this.createChatLunaLLMChainWrapper();
    try {
      await this.ctx.parallel(
        "chatluna/before-chat",
        arg.conversationId,
        arg.message,
        arg.variables,
        this,
        wrapper
      );
    } catch (error) {
      import_koishi_plugin_chatluna.logger.error("Something went wrong when calling before-chat hook:");
      import_koishi_plugin_chatluna.logger.error(error);
    }
    const additionalArgs = await this._chatHistory.getAdditionalArgs();
    if (arg.postHandler) {
      for (const key in arg.postHandler.variables) {
        arg.variables[key] = "";
      }
    }
    arg.variables = { ...additionalArgs, ...arg.variables };
    try {
      const response = await this.processChat(arg, wrapper);
      delete this._errorCountsMap[config.md5()];
      return response;
    } catch (error) {
      await this.handleChatError(error, config);
    }
  }
  async processChat(arg, wrapper) {
    const response = (await wrapper.call({
      ...arg,
      maxToken: (await this.preset)?.config?.maxOutputToken
    })).message;
    const displayResponse = new import_messages.AIMessage(response);
    displayResponse.additional_kwargs = response.additional_kwargs;
    this._chatCount++;
    if (arg.postHandler) {
      const handlerResult = await this.handlePostProcessing(
        arg,
        displayResponse
      );
      displayResponse.content = handlerResult.displayContent;
      await this._chatHistory.overrideAdditionalArgs(
        handlerResult.variables
      );
    }
    const messageContent = (0, import_string.getMessageContent)(displayResponse.content);
    if (messageContent.trim().length > 0) {
      await this.chatHistory.addMessage(arg.message);
      let saveMessage = response;
      if (!this.ctx.chatluna.config.rawOnCensor) {
        saveMessage = displayResponse;
      }
      await this.chatHistory.addMessage(saveMessage);
    }
    this.ctx.parallel(
      "chatluna/after-chat",
      arg.conversationId,
      arg.message,
      displayResponse,
      { ...arg.variables, chatCount: this._chatCount },
      this,
      wrapper
    );
    return { message: displayResponse };
  }
  async handlePostProcessing(arg, message) {
    import_koishi_plugin_chatluna.logger.debug(`original content: %c`, message.content);
    return await arg.postHandler.handler(
      arg.session,
      (0, import_string.getMessageContent)(message.content)
    );
  }
  async createChatLunaLLMChainWrapper() {
    const service = this.ctx.chatluna.platform;
    const [llmPlatform, llmModelName] = (0, import_count_tokens.parseRawModelName)(this._input.model);
    const currentLLMConfig = await service.randomConfig(llmPlatform);
    if (this._chains[currentLLMConfig.md5()]) {
      return [this._chains[currentLLMConfig.md5()], currentLLMConfig];
    }
    let embeddings;
    let llm;
    let modelInfo;
    let historyMemory;
    try {
      embeddings = await this._initEmbeddings(service);
    } catch (error) {
      if (error instanceof import_error.ChatLunaError) {
        throw error;
      }
      throw new import_error.ChatLunaError(
        import_error.ChatLunaErrorCode.EMBEDDINGS_INIT_ERROR,
        error
      );
    }
    try {
      ;
      [llm, modelInfo] = await this._initModel(
        service,
        currentLLMConfig.value,
        llmModelName
      );
    } catch (error) {
      if (error instanceof import_error.ChatLunaError) {
        throw error;
      }
      throw new import_error.ChatLunaError(import_error.ChatLunaErrorCode.MODEL_INIT_ERROR, error);
    }
    try {
      await this._createChatHistory();
    } catch (error) {
      if (error instanceof import_error.ChatLunaError) {
        throw error;
      }
      throw new import_error.ChatLunaError(
        import_error.ChatLunaErrorCode.CHAT_HISTORY_INIT_ERROR,
        error
      );
    }
    try {
      historyMemory = this._createHistoryMemory();
    } catch (error) {
      if (error instanceof import_error.ChatLunaError) {
        throw error;
      }
      throw new import_error.ChatLunaError(import_error.ChatLunaErrorCode.UNKNOWN_ERROR, error);
    }
    const chatChain = await service.createChatChain(this._input.chatMode, {
      botName: this._input.botName,
      model: llm,
      embeddings,
      historyMemory,
      preset: this._input.preset,
      vectorStoreName: this._input.vectorStoreName,
      supportChatChain: this._supportChatMode(modelInfo)
    });
    this._chains[currentLLMConfig.md5()] = chatChain;
    this._embeddings = embeddings;
    return [chatChain, currentLLMConfig];
  }
  get chatHistory() {
    return this._chatHistory;
  }
  get chatMode() {
    return this._input.chatMode;
  }
  get embeddings() {
    return this._embeddings;
  }
  get preset() {
    return this._input.preset();
  }
  async delete(ctx, room) {
    await this.clearChatHistory();
    for (const chain of Object.values(this._chains)) {
      await chain.model.clearContext(room.conversationId);
    }
    this._chains = {};
    await ctx.database.remove("chathub_conversation", {
      id: room.conversationId
    });
    await ctx.database.remove("chathub_room", {
      roomId: room.roomId
    });
    await ctx.database.remove("chathub_room_member", {
      roomId: room.roomId
    });
    await ctx.database.remove("chathub_room_group_member", {
      roomId: room.roomId
    });
    await ctx.database.remove("chathub_user", {
      defaultRoomId: room.roomId
    });
    await ctx.database.remove("chathub_message", {
      conversation: room.conversationId
    });
  }
  async clearChatHistory() {
    if (this._chatHistory == null) {
      await this._createChatHistory();
    }
    await this.ctx.root.parallel(
      "chatluna/clear-chat-history",
      this._input.conversationId,
      this
    );
    await this._chatHistory.clear();
    for (const chain of Object.values(this._chains)) {
      await chain.model.clearContext(this._input.conversationId);
    }
  }
  async _initEmbeddings(service) {
    if (this._input.embeddings == null || this._input.embeddings.length < 1 || this._input.embeddings === "无") {
      if (this._input.vectorStoreName != null && this._input.vectorStoreName?.length > 0 && this._input.vectorStoreName !== "无") {
        import_koishi_plugin_chatluna.logger.warn(
          "Embeddings are empty, falling back to fake embeddings. Try check your config."
        );
      }
      return import_in_memory.emptyEmbeddings;
    }
    const [platform, modelName] = (0, import_count_tokens.parseRawModelName)(this._input.embeddings);
    import_koishi_plugin_chatluna.logger.info(`init embeddings for %c`, this._input.embeddings);
    const client = await service.randomClient(platform);
    if (client == null || client instanceof import_client.PlatformModelClient) {
      import_koishi_plugin_chatluna.logger.warn(
        `Platform ${platform} is not supported, falling back to fake embeddings`
      );
      return import_in_memory.emptyEmbeddings;
    }
    if (client instanceof import_client.PlatformEmbeddingsClient) {
      return client.createModel(modelName);
    } else if (client instanceof import_client.PlatformModelAndEmbeddingsClient) {
      const model = client.createModel(modelName);
      if (model instanceof import_model.ChatLunaChatModel) {
        import_koishi_plugin_chatluna.logger.warn(
          `Model ${modelName} is not an embeddings model, falling back to fake embeddings`
        );
        return import_in_memory.emptyEmbeddings;
      }
      return model;
    }
  }
  async _initModel(service, config, llmModelName) {
    const platform = await service.getClient(config);
    const llmInfo = (await platform.getModels()).find(
      (model) => model.name === llmModelName
    );
    const llmModel = platform.createModel(llmModelName);
    if (llmModel instanceof import_model.ChatLunaChatModel) {
      return [llmModel, llmInfo];
    }
  }
  _supportChatMode(modelInfo) {
    if (
      // default check
      !modelInfo.supportMode?.includes(this._input.chatMode) && // all
      !modelInfo.supportMode?.includes("all") || // func call with plugin
      !modelInfo.functionCall && this._input.chatMode === "plugin"
    ) {
      import_koishi_plugin_chatluna.logger.warn(
        `Chat mode ${this._input.chatMode} is not supported by model ${this._input.model}`
      );
      return false;
    }
    return true;
  }
  async _createChatHistory() {
    if (this._chatHistory != null) {
      return this._chatHistory;
    }
    this._chatHistory = new import_message.KoishiChatMessageHistory(
      this.ctx,
      this._input.conversationId,
      this._input.maxMessagesCount
    );
    await this._chatHistory.loadConversation();
    return this._chatHistory;
  }
  _createHistoryMemory() {
    return new import_langchain.BufferMemory({
      returnMessages: true,
      inputKey: "input",
      outputKey: "output",
      chatHistory: this._chatHistory,
      humanPrefix: "user",
      aiPrefix: this._input.botName
    });
  }
};
function checkRange(times, delayTime) {
  const first = times[0];
  const last = times[times.length - 1];
  return last - first < delayTime;
}
__name(checkRange, "checkRange");
// Annotate the CommonJS export names for ESM import in node:
0 && (module.exports = {
  ChatInterface
});
