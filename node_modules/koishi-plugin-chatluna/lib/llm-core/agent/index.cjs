var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __name = (target, value) => __defProp(target, "name", { value, configurable: true });
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);

// src/llm-core/agent/index.ts
var agent_exports = {};
__export(agent_exports, {
  AgentActionOutputParser: () => AgentActionOutputParser,
  AgentExecutor: () => AgentExecutor,
  AgentExecutorIterator: () => AgentExecutorIterator,
  AgentMultiActionOutputParser: () => AgentMultiActionOutputParser,
  AgentRunnableSequence: () => AgentRunnableSequence,
  BaseAgent: () => BaseAgent,
  BaseMultiActionAgent: () => BaseMultiActionAgent,
  BaseSingleActionAgent: () => BaseSingleActionAgent,
  ExceptionTool: () => ExceptionTool,
  RunnableMultiActionAgent: () => RunnableMultiActionAgent,
  RunnableSingleActionAgent: () => RunnableSingleActionAgent,
  _formatIntermediateSteps: () => _formatIntermediateSteps,
  createOpenAIAgent: () => createOpenAIAgent,
  createReactAgent: () => createReactAgent,
  formatLogToString: () => formatLogToString,
  isRunnableAgent: () => isRunnableAgent
});
module.exports = __toCommonJS(agent_exports);

// src/llm-core/agent/openai/index.ts
var import_messages2 = require("@langchain/core/messages");
var import_runnables = require("@langchain/core/runnables");

// src/llm-core/agent/openai/output_parser.ts
var import_messages = require("@langchain/core/messages");
var import_output_parsers2 = require("@langchain/core/output_parsers");

// src/llm-core/agent/types.ts
var import_output_parsers = require("@langchain/core/output_parsers");
var AgentActionOutputParser = class extends import_output_parsers.BaseOutputParser {
  static {
    __name(this, "AgentActionOutputParser");
  }
};
var AgentMultiActionOutputParser = class extends import_output_parsers.BaseOutputParser {
  static {
    __name(this, "AgentMultiActionOutputParser");
  }
};

// src/llm-core/agent/openai/output_parser.ts
var OpenAIFunctionsAgentOutputParser = class extends AgentActionOutputParser {
  static {
    __name(this, "OpenAIFunctionsAgentOutputParser");
  }
  // eslint-disable-next-line @typescript-eslint/naming-convention
  lc_namespace = ["@langchain/core/messages", "agents", "openai"];
  // eslint-disable-next-line @typescript-eslint/naming-convention
  static lc_name() {
    return "OpenAIFunctionsAgentOutputParser";
  }
  async parse(text) {
    throw new Error(
      `OpenAIFunctionsAgentOutputParser can only parse messages.
Passed input: ${text}`
    );
  }
  async parseResult(generations) {
    if ("message" in generations[0] && (0, import_messages.isBaseMessage)(generations[0].message)) {
      return this.parseAIMessage(generations[0].message);
    }
    throw new Error(
      "parseResult on OpenAIFunctionsAgentOutputParser only works on ChatGeneration output"
    );
  }
  /**
   * Parses the output message into a FunctionsAgentAction or AgentFinish
   * object.
   * @param message The BaseMessage to parse.
   * @returns A FunctionsAgentAction or AgentFinish object.
   */
  parseAIMessage(message) {
    if (message.content && typeof message.content !== "string") {
      throw new Error(
        "This agent cannot parse non-string model responses."
      );
    }
    if (message.additional_kwargs.function_call) {
      const function_call = message.additional_kwargs.function_call;
      try {
        const toolInput = function_call.arguments ? JSON.parse(function_call.arguments) : {};
        return {
          tool: function_call.name,
          toolInput,
          log: message.content?.length > 0 ? message.content : `Invoking "${function_call.name}" with ${function_call.arguments ?? "{}"}`,
          messageLog: [message]
        };
      } catch (error) {
        throw new import_output_parsers2.OutputParserException(
          `Failed to parse function arguments from chat model response. Text: "${function_call.arguments}". ${error}`
        );
      }
    } else {
      return {
        returnValues: { output: message.content, message },
        log: message.content
      };
    }
  }
  getFormatInstructions() {
    throw new Error(
      "getFormatInstructions not implemented inside OpenAIFunctionsAgentOutputParser."
    );
  }
};
var OpenAIToolsAgentOutputParser = class extends AgentMultiActionOutputParser {
  static {
    __name(this, "OpenAIToolsAgentOutputParser");
  }
  // eslint-disable-next-line @typescript-eslint/naming-convention
  lc_namespace = ["@langchain/core/messages", "agents", "openai"];
  // eslint-disable-next-line @typescript-eslint/naming-convention
  static lc_name() {
    return "OpenAIToolsAgentOutputParser";
  }
  async parse(text) {
    throw new Error(
      `OpenAIFunctionsAgentOutputParser can only parse messages.
Passed input: ${text}`
    );
  }
  async parseResult(generations) {
    if ("message" in generations[0] && (0, import_messages.isBaseMessage)(generations[0].message)) {
      return this.parseAIMessage(generations[0].message);
    }
    throw new Error(
      "parseResult on OpenAIFunctionsAgentOutputParser only works on ChatGeneration output"
    );
  }
  /**
   * Parses the output message into a ToolsAgentAction[] or AgentFinish
   * object.
   * @param message The BaseMessage to parse.
   * @returns A ToolsAgentAction[] or AgentFinish object.
   */
  parseAIMessage(message) {
    if (message.content && typeof message.content !== "string") {
      throw new Error(
        "This agent cannot parse non-string model responses."
      );
    }
    if ((message instanceof import_messages.AIMessageChunk || message instanceof import_messages.AIMessage) && message.tool_calls != null) {
      const toolCalls = message.tool_calls;
      if (toolCalls.length < 1) {
        return {
          returnValues: { output: message.content },
          log: message.content
        };
      }
      try {
        return toolCalls.map((toolCall, i) => {
          const toolInput = toolCall.args;
          const messageLog = i === 0 ? [message] : [];
          return {
            tool: toolCall.name,
            toolInput,
            toolCallId: toolCall.id,
            log: message.content?.length > 0 ? message.content : `Invoking "${toolCall.name}" with ${JSON.stringify(toolCall.args) ?? "{}"}`,
            messageLog
          };
        });
      } catch (error) {
        throw new import_output_parsers2.OutputParserException(
          `Failed to parse tool arguments from chat model response. Text: "${JSON.stringify(
            toolCalls
          )}". ${error}`
        );
      }
    } else if (message.additional_kwargs.tool_calls) {
      const toolCalls = message.additional_kwargs.tool_calls;
      try {
        return toolCalls.map((toolCall, i) => {
          const toolInput = toolCall.function.arguments ? JSON.parse(toolCall.function.arguments) : {};
          const messageLog = i === 0 ? [message] : [];
          return {
            tool: toolCall.function.name,
            toolInput,
            toolCallId: toolCall.id,
            log: message.content?.length > 0 ? message.content : `Invoking "${toolCall.function.name}" with ${toolCall.function.arguments ?? "{}"}`,
            messageLog
          };
        });
      } catch (error) {
        throw new import_output_parsers2.OutputParserException(
          `Failed to parse tool arguments from chat model response. Text: "${JSON.stringify(
            toolCalls
          )}". ${error}`
        );
      }
    } else {
      return {
        returnValues: { output: message.content },
        log: message.content
      };
    }
  }
  getFormatInstructions() {
    throw new Error(
      "getFormatInstructions not implemented inside OpenAIToolsAgentOutputParser."
    );
  }
};

// src/llm-core/agent/openai/index.ts
function isFunctionsAgentAction(action) {
  return action.messageLog !== void 0;
}
__name(isFunctionsAgentAction, "isFunctionsAgentAction");
function isToolsAgentAction(action) {
  return action.toolCallId !== void 0;
}
__name(isToolsAgentAction, "isToolsAgentAction");
function _convertAgentStepToMessages(action, observation) {
  if (isToolsAgentAction(action) && action.toolCallId !== void 0) {
    const log = action.messageLog;
    if (observation.length < 1) {
      observation = `The tool ${action.tool} returned no output.`;
    }
    return log.concat(
      new import_messages2.ToolMessage({
        content: observation,
        name: action.tool,
        tool_call_id: action.toolCallId
      })
    );
  } else if (isFunctionsAgentAction(action) && action.messageLog !== void 0) {
    return action.messageLog?.concat(
      new import_messages2.FunctionMessage(observation, action.tool)
    );
  } else {
    return [new import_messages2.AIMessage(action.log)];
  }
}
__name(_convertAgentStepToMessages, "_convertAgentStepToMessages");
function _formatIntermediateSteps(intermediateSteps) {
  return intermediateSteps.flatMap(
    ({ action, observation }) => _convertAgentStepToMessages(action, observation)
  );
}
__name(_formatIntermediateSteps, "_formatIntermediateSteps");
function createOpenAIAgent({
  llm,
  tools,
  prompt
}) {
  const llmWithTools = llm.bind({
    tools
  });
  let outputParser = new OpenAIToolsAgentOutputParser();
  const agent = import_runnables.RunnableSequence.from([
    import_runnables.RunnablePassthrough.assign({
      // eslint-disable-next-line @typescript-eslint/naming-convention
      agent_scratchpad: /* @__PURE__ */ __name((input) => _formatIntermediateSteps(input.steps), "agent_scratchpad")
      /* // @ts-expect-error eslint-disable-next-line @typescript-eslint/naming-convention
      input_text: (input: { input: BaseMessage[] }) =>
          getMessageContent(input.input[0].content) */
    }),
    prompt,
    llmWithTools,
    import_runnables.RunnableLambda.from((input) => {
      if ((input?.additional_kwargs?.tool_calls || (input instanceof import_messages2.AIMessageChunk || input instanceof import_messages2.AIMessage) && input.tool_calls) && outputParser instanceof OpenAIFunctionsAgentOutputParser) {
        outputParser = new OpenAIToolsAgentOutputParser();
      } else if (input?.additional_kwargs?.function_call && outputParser instanceof OpenAIToolsAgentOutputParser) {
        outputParser = new OpenAIFunctionsAgentOutputParser();
      }
      return outputParser.parseResult([
        {
          message: input,
          text: input.content
        }
      ]);
    })
  ]);
  return agent;
}
__name(createOpenAIAgent, "createOpenAIAgent");

// src/llm-core/agent/agent.ts
var import_serializable = require("@langchain/core/load/serializable");
var import_runnables2 = require("@langchain/core/runnables");
var BaseAgent = class extends import_serializable.Serializable {
  static {
    __name(this, "BaseAgent");
  }
  get returnValues() {
    return ["output"];
  }
  get allowedTools() {
    return void 0;
  }
  /**
   * Return the string type key uniquely identifying this class of agent.
   */
  _agentType() {
    throw new Error("Not implemented");
  }
  /**
   * Return response when agent has been stopped due to max iterations
   */
  returnStoppedResponse(earlyStoppingMethod, _steps, _inputs, _callbackManager) {
    if (earlyStoppingMethod === "force") {
      return Promise.resolve({
        returnValues: {
          output: "Agent stopped due to max iterations."
        },
        log: ""
      });
    }
    throw new Error(`Invalid stopping method: ${earlyStoppingMethod}`);
  }
  /**
   * Prepare the agent for output, if needed
   */
  async prepareForOutput(_returnValues, _steps) {
    return {};
  }
};
var BaseSingleActionAgent = class extends BaseAgent {
  static {
    __name(this, "BaseSingleActionAgent");
  }
  _agentActionType() {
    return "single";
  }
};
var BaseMultiActionAgent = class extends BaseAgent {
  static {
    __name(this, "BaseMultiActionAgent");
  }
  _agentActionType() {
    return "multi";
  }
};
function isAgentAction(input) {
  return !Array.isArray(input) && input?.tool !== void 0;
}
__name(isAgentAction, "isAgentAction");
function isRunnableAgent(x) {
  return x.runnable !== void 0;
}
__name(isRunnableAgent, "isRunnableAgent");
var AgentRunnableSequence = class extends import_runnables2.RunnableSequence {
  static {
    __name(this, "AgentRunnableSequence");
  }
  streamRunnable;
  singleAction;
  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  static fromRunnables([first, ...runnables], config) {
    const sequence = import_runnables2.RunnableSequence.from(
      [first, ...runnables],
      config.name
    );
    sequence.singleAction = config.singleAction;
    sequence.streamRunnable = config.streamRunnable;
    return sequence;
  }
  static isAgentRunnableSequence(x) {
    return typeof x.singleAction === "boolean";
  }
};
var RunnableSingleActionAgent = class extends BaseSingleActionAgent {
  static {
    __name(this, "RunnableSingleActionAgent");
  }
  lc_namespace = ["langchain", "agents", "runnable"];
  runnable;
  get inputKeys() {
    return [];
  }
  /**
   * Whether to stream from the runnable or not.
   * If true, the underlying LLM is invoked in a streaming fashion to make it
   * possible to get access to the individual LLM tokens when using
   * `streamLog` with the Agent Executor. If false then LLM is invoked in a
   * non-streaming fashion and individual LLM tokens will not be available
   * in `streamLog`.
   *
   * Note that the runnable should still only stream a single action or
   * finish chunk.
   */
  streamRunnable = true;
  defaultRunName = "RunnableAgent";
  constructor(fields) {
    super(fields);
    this.runnable = fields.runnable;
    this.defaultRunName = fields.defaultRunName ?? this.runnable.name ?? this.defaultRunName;
    this.streamRunnable = fields.streamRunnable ?? this.streamRunnable;
  }
  async plan(steps, inputs, callbackManager, config) {
    const combinedInput = { ...inputs, steps };
    const combinedConfig = (0, import_runnables2.patchConfig)(config, {
      callbacks: callbackManager,
      runName: this.defaultRunName
    });
    if (this.streamRunnable) {
      const stream = await this.runnable.stream(
        combinedInput,
        combinedConfig
      );
      let finalOutput;
      for await (const chunk of stream) {
        if (finalOutput === void 0) {
          finalOutput = chunk;
        } else {
          throw new Error(
            [
              `Multiple agent actions/finishes received in streamed agent output.`,
              `Set "streamRunnable: false" when initializing the agent to invoke this agent in non-streaming mode.`
            ].join("\n")
          );
        }
      }
      if (finalOutput === void 0) {
        throw new Error(
          [
            "No streaming output received from underlying runnable.",
            `Set "streamRunnable: false" when initializing the agent to invoke this agent in non-streaming mode.`
          ].join("\n")
        );
      }
      return finalOutput;
    } else {
      return this.runnable.invoke(combinedInput, combinedConfig);
    }
  }
};
var RunnableMultiActionAgent = class extends BaseMultiActionAgent {
  static {
    __name(this, "RunnableMultiActionAgent");
  }
  lc_namespace = ["langchain", "agents", "runnable"];
  // TODO: Rename input to "intermediate_steps"
  runnable;
  defaultRunName = "RunnableAgent";
  stop;
  streamRunnable = true;
  get inputKeys() {
    return [];
  }
  constructor(fields) {
    super(fields);
    this.runnable = fields.runnable;
    this.stop = fields.stop;
    this.defaultRunName = fields.defaultRunName ?? this.runnable.name ?? this.defaultRunName;
    this.streamRunnable = fields.streamRunnable ?? this.streamRunnable;
  }
  async plan(steps, inputs, callbackManager, config) {
    const combinedInput = { ...inputs, steps };
    const combinedConfig = (0, import_runnables2.patchConfig)(config, {
      callbacks: callbackManager,
      runName: this.defaultRunName
    });
    let output;
    if (this.streamRunnable) {
      const stream = await this.runnable.stream(
        combinedInput,
        combinedConfig
      );
      let finalOutput;
      for await (const chunk of stream) {
        if (finalOutput === void 0) {
          finalOutput = chunk;
        } else {
          throw new Error(
            [
              `Multiple agent actions/finishes received in streamed agent output.`,
              `Set "streamRunnable: false" when initializing the agent to invoke this agent in non-streaming mode.`
            ].join("\n")
          );
        }
      }
      if (finalOutput === void 0) {
        throw new Error(
          [
            "No streaming output received from underlying runnable.",
            `Set "streamRunnable: false" when initializing the agent to invoke this agent in non-streaming mode.`
          ].join("\n")
        );
      }
      output = finalOutput;
    } else {
      output = await this.runnable.invoke(combinedInput, combinedConfig);
    }
    if (isAgentAction(output)) {
      return [output];
    }
    return output;
  }
};

// src/llm-core/agent/executor.ts
var import_tools = require("@langchain/core/tools");
var import_runnables3 = require("@langchain/core/runnables");
var import_manager = require("@langchain/core/callbacks/manager");
var import_output_parsers3 = require("@langchain/core/output_parsers");
var import_base = require("koishi-plugin-chatluna/llm-core/chain/base");
var import_serializable2 = require("@langchain/core/load/serializable");
var AgentExecutorIterator = class extends import_serializable2.Serializable {
  static {
    __name(this, "AgentExecutorIterator");
  }
  lc_namespace = ["langchain", "agents", "executor_iterator"];
  agentExecutor;
  inputs;
  config;
  /** @deprecated Use "config" */
  callbacks;
  /** @deprecated Use "config" */
  tags;
  /** @deprecated Use "config" */
  metadata;
  /** @deprecated Use "config" */
  runName;
  _finalOutputs;
  get finalOutputs() {
    return this._finalOutputs;
  }
  /** Intended to be used as a setter method, needs to be async. */
  async setFinalOutputs(value) {
    this._finalOutputs = void 0;
    if (value) {
      const preparedOutputs = await this.agentExecutor.prepOutputs(this.inputs, value, true);
      this._finalOutputs = preparedOutputs;
    }
  }
  runManager;
  intermediateSteps = [];
  iterations = 0;
  get nameToToolMap() {
    const toolMap = this.agentExecutor.tools.map((tool) => ({
      [tool.name]: tool
    }));
    return Object.assign({}, ...toolMap);
  }
  constructor(fields) {
    super(fields);
    this.agentExecutor = fields.agentExecutor;
    this.inputs = fields.inputs;
    this.callbacks = fields.callbacks;
    this.tags = fields.tags;
    this.metadata = fields.metadata;
    this.runName = fields.runName;
    this.runManager = fields.runManager;
    this.config = fields.config;
  }
  /**
   * Reset the iterator to its initial state, clearing intermediate steps,
   * iterations, and the final output.
   */
  reset() {
    this.intermediateSteps = [];
    this.iterations = 0;
    this._finalOutputs = void 0;
  }
  updateIterations() {
    this.iterations += 1;
  }
  async *streamIterator() {
    this.reset();
    while (true) {
      try {
        if (this.iterations === 0) {
          await this.onFirstStep();
        }
        const result = await this._callNext();
        yield result;
      } catch (e) {
        if ("message" in e && e.message.startsWith("Final outputs already reached: ")) {
          if (!this.finalOutputs) {
            throw e;
          }
          return this.finalOutputs;
        }
        if (this.runManager) {
          await this.runManager.handleChainError(e);
        }
        throw e;
      }
    }
  }
  /**
   * Perform any necessary setup for the first step
   * of the asynchronous iterator.
   */
  async onFirstStep() {
    if (this.iterations === 0) {
      const callbackManager = import_manager.CallbackManager.configure(
        this.callbacks ?? this.config?.callbacks,
        this.agentExecutor.callbacks,
        this.tags ?? this.config?.tags,
        this.agentExecutor.tags,
        this.metadata ?? this.config?.metadata,
        this.agentExecutor.metadata,
        {
          verbose: this.agentExecutor.verbose
        }
      );
      this.runManager = await callbackManager?.handleChainStart(
        this.agentExecutor.toJSON(),
        this.inputs,
        this.config?.runId,
        void 0,
        this.tags ?? this.config?.tags,
        this.metadata ?? this.config?.metadata,
        this.runName ?? this.config?.runName
      );
      if (this.config !== void 0) {
        delete this.config.runId;
      }
    }
  }
  /**
   * Execute the next step in the chain using the
   * AgentExecutor's _takeNextStep method.
   */
  async _executeNextStep(runManager) {
    return this.agentExecutor._takeNextStep(
      this.nameToToolMap,
      this.inputs,
      this.intermediateSteps,
      runManager,
      this.config
    );
  }
  /**
   * Process the output of the next step,
   * handling AgentFinish and tool return cases.
   */
  async _processNextStepOutput(nextStepOutput, runManager) {
    if ("returnValues" in nextStepOutput) {
      const output2 = await this.agentExecutor._return(
        nextStepOutput,
        this.intermediateSteps,
        runManager
      );
      if (this.runManager) {
        await this.runManager.handleChainEnd(output2);
      }
      await this.setFinalOutputs(output2);
      return output2;
    }
    this.intermediateSteps = this.intermediateSteps.concat(
      nextStepOutput
    );
    let output = {};
    if (Array.isArray(nextStepOutput) && nextStepOutput.length === 1) {
      const nextStep = nextStepOutput[0];
      const toolReturn = await this.agentExecutor._getToolReturn(nextStep);
      if (toolReturn) {
        output = await this.agentExecutor._return(
          toolReturn,
          this.intermediateSteps,
          runManager
        );
        await this.runManager?.handleChainEnd(output);
        await this.setFinalOutputs(output);
      }
    }
    output = { intermediateSteps: nextStepOutput };
    return output;
  }
  async _stop() {
    const output = await this.agentExecutor.agent.returnStoppedResponse(
      this.agentExecutor.earlyStoppingMethod,
      this.intermediateSteps,
      this.inputs
    );
    const returnedOutput = await this.agentExecutor._return(
      output,
      this.intermediateSteps,
      this.runManager
    );
    await this.setFinalOutputs(returnedOutput);
    await this.runManager?.handleChainEnd(returnedOutput);
    return returnedOutput;
  }
  async _callNext() {
    if (this.finalOutputs) {
      throw new Error(
        `Final outputs already reached: ${JSON.stringify(
          this.finalOutputs,
          null,
          2
        )}`
      );
    }
    if (!this.agentExecutor.shouldContinueGetter(this.iterations)) {
      return this._stop();
    }
    const nextStepOutput = await this._executeNextStep(this.runManager);
    const output = await this._processNextStepOutput(
      nextStepOutput,
      this.runManager
    );
    this.updateIterations();
    return output;
  }
};
var ExceptionTool = class extends import_tools.Tool {
  static {
    __name(this, "ExceptionTool");
  }
  name = "_Exception";
  description = "Exception tool";
  async _call(query) {
    return query;
  }
};
var AgentExecutor = class _AgentExecutor extends import_base.BaseChain {
  static {
    __name(this, "AgentExecutor");
  }
  static lc_name() {
    return "AgentExecutor";
  }
  get lc_namespace() {
    return ["langchain", "agents", "executor"];
  }
  agent;
  tools;
  returnIntermediateSteps = false;
  maxIterations = 15;
  earlyStoppingMethod = "force";
  // TODO: Update BaseChain implementation on breaking change to include this
  returnOnlyOutputs = true;
  /**
       * How to handle errors raised by the agent's output parser.
        Defaults to `False`, which raises the error.
  
        If `true`, the error will be sent back to the LLM as an observation.
        If a string, the string itself will be sent to the LLM as an observation.
        If a callable function, the function will be called with the exception
        as an argument, and the result of that function will be passed to the agent
        as an observation.
       */
  handleParsingErrors = false;
  handleToolRuntimeErrors;
  get inputKeys() {
    return this.agent.inputKeys;
  }
  get outputKeys() {
    return this.agent.returnValues;
  }
  constructor(input) {
    let agent;
    let returnOnlyOutputs = true;
    if (import_runnables3.Runnable.isRunnable(input.agent)) {
      if (AgentRunnableSequence.isAgentRunnableSequence(input.agent)) {
        if (input.agent.singleAction) {
          agent = new RunnableSingleActionAgent({
            runnable: input.agent,
            streamRunnable: input.agent.streamRunnable
          });
        } else {
          agent = new RunnableMultiActionAgent({
            runnable: input.agent,
            streamRunnable: input.agent.streamRunnable
          });
        }
      } else {
        agent = new RunnableMultiActionAgent({ runnable: input.agent });
      }
      returnOnlyOutputs = false;
    } else {
      if (isRunnableAgent(input.agent)) {
        returnOnlyOutputs = false;
      }
      agent = input.agent;
    }
    super(input);
    this.agent = agent;
    this.tools = input.tools;
    this.handleParsingErrors = input.handleParsingErrors ?? this.handleParsingErrors;
    this.handleToolRuntimeErrors = input.handleToolRuntimeErrors;
    this.returnOnlyOutputs = returnOnlyOutputs;
    if (this.agent._agentActionType() === "multi") {
      for (const tool of this.tools) {
        if (tool.returnDirect) {
          throw new Error(
            `Tool with return direct ${tool.name} not supported for multi-action agent.`
          );
        }
      }
    }
    this.returnIntermediateSteps = input.returnIntermediateSteps ?? this.returnIntermediateSteps;
    this.maxIterations = input.maxIterations ?? this.maxIterations;
    this.earlyStoppingMethod = input.earlyStoppingMethod ?? this.earlyStoppingMethod;
  }
  /** Create from agent and a list of tools. */
  static fromAgentAndTools(fields) {
    return new _AgentExecutor(fields);
  }
  get shouldContinueGetter() {
    return this.shouldContinue.bind(this);
  }
  /**
   * Method that checks if the agent execution should continue based on the
   * number of iterations.
   * @param iterations The current number of iterations.
   * @returns A boolean indicating whether the agent execution should continue.
   */
  shouldContinue(iterations) {
    return this.maxIterations === void 0 || iterations < this.maxIterations;
  }
  /** @ignore */
  async _call(inputs, runManager, config) {
    const toolsByName = Object.fromEntries(
      this.tools.map((t) => [t.name.toLowerCase(), t])
    );
    const steps = [];
    let iterations = 0;
    const signal = config?.signal;
    const getOutput = /* @__PURE__ */ __name(async (finishStep) => {
      const { returnValues } = finishStep;
      const additional = await this.agent.prepareForOutput(
        returnValues,
        steps
      );
      await runManager?.handleAgentEnd(finishStep);
      let response;
      if (this.returnIntermediateSteps) {
        response = {
          ...returnValues,
          intermediateSteps: steps,
          ...additional
        };
      } else {
        response = { ...returnValues, ...additional };
      }
      if (!this.returnOnlyOutputs) {
        response = { ...inputs, ...response };
      }
      return response;
    }, "getOutput");
    while (this.shouldContinue(iterations) && !(signal?.aborted ?? false)) {
      let output;
      try {
        output = await this.agent.plan(
          steps,
          inputs,
          runManager?.getChild(),
          config
        );
      } catch (e) {
        if (e instanceof import_output_parsers3.OutputParserException) {
          let observation;
          let text = e.message;
          if (this.handleParsingErrors === true) {
            if (e.sendToLLM) {
              observation = e.observation;
              text = e.llmOutput ?? "";
            } else {
              observation = "Invalid or incomplete response";
            }
          } else if (typeof this.handleParsingErrors === "string") {
            observation = this.handleParsingErrors;
          } else if (typeof this.handleParsingErrors === "function") {
            observation = this.handleParsingErrors(e);
          } else {
            throw e;
          }
          output = {
            tool: "_Exception",
            toolInput: observation,
            log: text
          };
        } else {
          throw e;
        }
      }
      if ("returnValues" in output) {
        return getOutput(output);
      }
      let actions;
      if (Array.isArray(output)) {
        actions = output;
      } else {
        actions = [output];
      }
      const newSteps = await Promise.all(
        actions.map(async (action) => {
          await runManager?.handleAgentAction(action);
          const tool = action.tool === "_Exception" ? new ExceptionTool() : toolsByName[action.tool?.toLowerCase()];
          let observation;
          try {
            observation = tool ? await tool.invoke(
              action.toolInput,
              (0, import_runnables3.patchConfig)(config, {
                callbacks: runManager?.getChild()
              })
            ) : `${action.tool} is not a valid tool, try another one.`;
            if (typeof observation !== "string") {
              throw new Error(
                "Received unsupported non-string response from tool call."
              );
            }
          } catch (e) {
            if (e instanceof import_tools.ToolInputParsingException) {
              if (this.handleParsingErrors === true) {
                observation = "Invalid or incomplete tool input. Please try again.";
              } else if (typeof this.handleParsingErrors === "string") {
                observation = this.handleParsingErrors;
              } else if (typeof this.handleParsingErrors === "function") {
                observation = this.handleParsingErrors(e);
              } else {
                throw e;
              }
              observation = await new ExceptionTool().invoke(
                observation,
                runManager?.getChild()
              );
              return { action, observation: observation ?? "" };
            } else if (this.handleToolRuntimeErrors !== void 0) {
              observation = this.handleToolRuntimeErrors(e);
            }
          }
          return { action, observation: observation ?? "" };
        })
      );
      steps.push(...newSteps);
      const lastStep = steps[steps.length - 1];
      const lastTool = toolsByName[lastStep.action.tool?.toLowerCase()];
      if (lastTool?.returnDirect) {
        return getOutput({
          returnValues: {
            [this.agent.returnValues[0]]: lastStep.observation
          },
          log: ""
        });
      }
      iterations += 1;
    }
    const finish = await this.agent.returnStoppedResponse(
      this.earlyStoppingMethod,
      steps,
      inputs
    );
    return getOutput(finish);
  }
  async _takeNextStep(nameToolMap, inputs, intermediateSteps, runManager, config) {
    let output;
    try {
      output = await this.agent.plan(
        intermediateSteps,
        inputs,
        runManager?.getChild(),
        config
      );
    } catch (e) {
      if (e instanceof import_output_parsers3.OutputParserException) {
        let observation;
        let text = e.message;
        if (this.handleParsingErrors === true) {
          if (e.sendToLLM) {
            observation = e.observation;
            text = e.llmOutput ?? "";
          } else {
            observation = "Invalid or incomplete response";
          }
        } else if (typeof this.handleParsingErrors === "string") {
          observation = this.handleParsingErrors;
        } else if (typeof this.handleParsingErrors === "function") {
          observation = this.handleParsingErrors(e);
        } else {
          throw e;
        }
        output = {
          tool: "_Exception",
          toolInput: observation,
          log: text
        };
      } else {
        throw e;
      }
    }
    if ("returnValues" in output) {
      return output;
    }
    let actions;
    if (Array.isArray(output)) {
      actions = output;
    } else {
      actions = [output];
    }
    const result = [];
    for (const agentAction of actions) {
      let observation = "";
      if (runManager) {
        await runManager?.handleAgentAction(agentAction);
      }
      if (agentAction.tool in nameToolMap) {
        const tool = nameToolMap[agentAction.tool];
        try {
          observation = await tool.invoke(
            agentAction.toolInput,
            runManager?.getChild()
          );
          if (typeof observation !== "string") {
            throw new Error(
              "Received unsupported non-string response from tool call."
            );
          }
        } catch (e) {
          if (e instanceof import_tools.ToolInputParsingException) {
            if (this.handleParsingErrors === true) {
              observation = "Invalid or incomplete tool input. Please try again.";
            } else if (typeof this.handleParsingErrors === "string") {
              observation = this.handleParsingErrors;
            } else if (typeof this.handleParsingErrors === "function") {
              observation = this.handleParsingErrors(e);
            } else {
              throw e;
            }
            observation = await new ExceptionTool().invoke(
              observation,
              runManager?.getChild()
            );
          }
        }
      } else {
        observation = `${agentAction.tool} is not a valid tool, try another available tool: ${Object.keys(
          nameToolMap
        ).join(", ")}`;
      }
      result.push({
        action: agentAction,
        observation
      });
    }
    return result;
  }
  async _return(output, intermediateSteps, runManager) {
    if (runManager) {
      await runManager.handleAgentEnd(output);
    }
    const finalOutput = output.returnValues;
    if (this.returnIntermediateSteps) {
      finalOutput.intermediateSteps = intermediateSteps;
    }
    return finalOutput;
  }
  async _getToolReturn(nextStepOutput) {
    const { action, observation } = nextStepOutput;
    const nameToolMap = Object.fromEntries(
      this.tools.map((t) => [t.name.toLowerCase(), t])
    );
    const [returnValueKey = "output"] = this.agent.returnValues;
    if (action.tool in nameToolMap) {
      if (nameToolMap[action.tool].returnDirect) {
        return {
          returnValues: { [returnValueKey]: observation },
          log: ""
        };
      }
    }
    return null;
  }
  _returnStoppedResponse(earlyStoppingMethod) {
    if (earlyStoppingMethod === "force") {
      return {
        returnValues: {
          output: "Agent stopped due to iteration limit or time limit."
        },
        log: ""
      };
    }
    throw new Error(
      `Got unsupported early_stopping_method: ${earlyStoppingMethod}`
    );
  }
  async *_streamIterator(inputs, options) {
    const agentExecutorIterator = new AgentExecutorIterator({
      inputs,
      agentExecutor: this,
      config: options,
      // TODO: Deprecate these other parameters
      metadata: options?.metadata,
      tags: options?.tags,
      callbacks: options?.callbacks
    });
    const iterator = agentExecutorIterator.streamIterator();
    for await (const step of iterator) {
      if (!step) {
        continue;
      }
      yield step;
    }
  }
  _chainType() {
    return "agent_executor";
  }
};

// src/llm-core/agent/react/index.ts
var import_prompts2 = require("@langchain/core/prompts");
var import_runnables4 = require("@langchain/core/runnables");

// src/llm-core/agent/react/output_parser.ts
var import_prompts = require("@langchain/core/prompts");
var import_output_parsers4 = require("@langchain/core/output_parsers");

// src/llm-core/agent/react/prompt.ts
var FORMAT_INSTRUCTIONS = `You are an expert assistant who can solve any task using tool calls. You will be given a task to solve as best you can.
To do so, you have been given access to the following tools: {tool_names}

The tool calls you write are actions: after the tools are executed, you will get the results of the tool calls as "observations".
At each step, you should first explain your reasoning towards solving the task and the tools that you want to use within <thought> tags.
Then you should write one or more valid JSON tool calls within <tool_calling> tags.
This Thought/Tool_calling/Observation cycle can repeat N times, you should take several steps when needed.

Here are the output format:

<thought>
Your reasoning and thought process for the current step
</thought>

<tool_calling>
[
  {{
    "name": "tool_name",
    "arguments": {{"param1": "value1", "param2": "value2"}}
   }}
]
</tool_calling>

You can call multiple tools at once by including multiple tool objects in the JSON array within the <tool_calling> tags.

ONLY output within the <thought> and <tool_calling> sequences. You will get the Observation from the tool calls. Do not output the Observation yourself.

To provide the final answer to the task, use a tool call with "name": "final_answer". It is the only way to complete the task, else you will be stuck on a loop. So your final output should look like this:

<thought>
I have gathered all the necessary information and can now provide the final answer.
</thought>

<tool_calling>
[
  {{
    "name": "final_answer",
    "arguments": {{"answer": "insert your final answer here"}}
  }}
]
</tool_calling>

Here are a few examples using notional tools:
---
Task: "Generate an image of the oldest person in this document and also search for their biography."

<thought>
I need to first find out who is the oldest person in the document, then generate an image of them and search for their biography. I'll start by using the document_qa tool to find the oldest person.
</thought>

<tool_calling>
[
  {{
    "name": "document_qa",
    "arguments": {{"document": "document.pdf", "question": "Who is the oldest person mentioned?"}}
  }}
]
</tool_calling>

Observation: "The oldest person in the document is John Doe, a 55 year old lumberjack living in Newfoundland."

<thought>
Now I know the oldest person is John Doe, a 55-year-old lumberjack from Newfoundland. I can generate an image of him and search for his biography at the same time.
</thought>

<tool_calling>
[
 {{
    "name": "image_generator",
    "arguments": {{"prompt": "A portrait of John Doe, a 55-year-old lumberjack living in Newfoundland, Canada"}}
  }},
  {{
    "name": "search",
    "arguments": {{"query": "John Doe lumberjack Newfoundland biography"}}
  }}
]
</tool_calling>

Observation: "image.png" and "John Doe is a well-known lumberjack in Newfoundland with over 30 years of experience in forestry."

<thought>
I have successfully generated an image and found biographical information. Now I can provide the final answer with both pieces of information.
</thought>

<tool_calling>
[
  {{
    "name": "final_answer",
    "arguments": {{"answer": "I have generated an image (image.png) of John Doe, the oldest person in the document, and found that he is a well-known lumberjack in Newfoundland with over 30 years of experience in forestry."}}
  }}
]
</tool_calling>

---
Task: "What is the result of the following operation: 5 + 3 + 1294.678?"

<thought>
I need to calculate the sum of 5 + 3 + 1294.678. I'll use the python_interpreter tool to execute this calculation.
</thought>

<tool_calling>
[
  {{
    "name": "python_interpreter",
    "arguments": {{"code": "5 + 3 + 1294.678"}}
  }}
]
</tool_calling>

Observation: 1302.678

<thought>
I have calculated the result. Now I can provide the final answer.
</thought>

<tool_calling>
[
  {{
    "name": "final_answer",
    "arguments": {{"answer": "1302.678"}}
  }}
]
</tool_calling>

---
Task: "Which city has the highest population, Guangzhou or Shanghai?"

<thought>
I need to search for the population data of both Guangzhou and Shanghai to compare them. I'll search for both cities' population information simultaneously.
</thought>

<tool_calling>
[
  {{
    "name": "search",
    "arguments": {{"query": "Population Guangzhou 2023"}}
  }},
  {{
    "name": "search",
    "arguments": {{"query": "Population Shanghai 2023"}}
  }}
]
</tool_calling>

Observation: ['Guangzhou has a population of 15 million inhabitants as of 2021.'] and ['Shanghai has a population of 26 million (2019)']

<thought>
Based on the search results, Shanghai has a population of 26 million while Guangzhou has 15 million. Therefore, Shanghai has the higher population.
</thought>

<tool_calling>
[
  {{
    "name": "final_answer",
    "arguments": {{"answer": "Shanghai has the highest population with 26 million people, compared to Guangzhou's 15 million people."}}
  }}
]
</tool_calling>

Above examples were using notional tools that might not exist for you. You only have access to these tools:

{tool_descriptions}

Here are the rules you should always follow to solve your task:
1. ALWAYS provide tool calls within <tool_calling> tags, else you will fail.
2. Always use the right arguments for the tools. Never use variable names as the action arguments, use the actual values instead.
3. You can call multiple tools at once if it makes sense for efficiency.
4. Call tools only when needed: do not call the search agent if you do not need information, try to solve the task yourself.
If no tool call is needed, use final_answer tool to return your answer.
5. Never re-do a tool call that you previously did with the exact same parameters.
6. Always include your reasoning in <thought> tags before making tool calls.

Now Begin! If you solve the task correctly, you will receive a reward of $1,000,000.`;

// src/llm-core/agent/react/output_parser.ts
var ReActMultiInputOutputParser = class extends AgentMultiActionOutputParser {
  static {
    __name(this, "ReActMultiInputOutputParser");
  }
  lc_namespace = ["langchain", "agents", "react"];
  toolNames;
  constructor(fields) {
    super(fields);
    this.toolNames = fields.toolNames;
  }
  /**
   * Parses the given text into an AgentAction[] or AgentFinish object.
   * @param text Text to parse.
   * @returns Promise that resolves to an AgentAction[] or AgentFinish object.
   */
  async parse(text) {
    const thoughtRegex = /<thought>(.*?)<\/thought>/s;
    const toolCallingRegex = /<tool_calling>(.*?)<\/tool_calling>/s;
    const thoughtMatch = text.match(thoughtRegex);
    const toolCallingMatch = text.match(toolCallingRegex);
    if (toolCallingMatch) {
      const [, toolCallingContent] = toolCallingMatch;
      const [, thoughts] = thoughtMatch || ["", ""];
      const cleanedContent = toolCallingContent.trim();
      return this.parseActions(cleanedContent, thoughts.trim());
    }
    throw new import_output_parsers4.OutputParserException(
      `Could not parse LLM output: ${text}`,
      `Could not parse LLM output: ${text}`,
      `Could not parse LLM output: ${text}`,
      true
    );
  }
  parseActions(toolCallingContent, thoughts) {
    try {
      const parsedActions = JSON.parse(toolCallingContent);
      if (!Array.isArray(parsedActions)) {
        throw new import_output_parsers4.OutputParserException(
          `Tool calling content must be an array: ${toolCallingContent}`
        );
      }
      const finalAnswerAction = parsedActions.find(
        (action) => action.name === "final_answer"
      );
      if (finalAnswerAction) {
        return {
          returnValues: { output: finalAnswerAction.arguments },
          log: thoughts
        };
      }
      return parsedActions.map((action) => {
        if (action.name == null || action.arguments == null) {
          throw new import_output_parsers4.OutputParserException(
            `Invalid action format: ${JSON.stringify(action)}`
          );
        }
        return {
          tool: action.name,
          toolInput: action.arguments || {},
          log: thoughts
        };
      });
    } catch (e) {
      throw new import_output_parsers4.OutputParserException(
        `Could not parse tool calling content: ${toolCallingContent}. Error: ${e}`
      );
    }
  }
  /**
   * Returns the format instructions as a string.
   * @param options Options for getting the format instructions.
   * @returns Format instructions as a string.
   */
  getFormatInstructions() {
    return (0, import_prompts.renderTemplate)(FORMAT_INSTRUCTIONS, "f-string", {
      tool_names: this.toolNames.join(", ")
    });
  }
};

// src/llm-core/agent/react/index.ts
var import_agent2 = require("koishi-plugin-chatluna/llm-core/agent");

// src/llm-core/agent/render.ts
var import_zod_to_json_schema = require("zod-to-json-schema");
var import_base2 = require("@langchain/core/language_models/base");
function renderTextDescriptionAndArgs(tools) {
  if (tools.every(import_base2.isOpenAITool)) {
    return tools.map(
      (tool) => `${tool.function.name}${tool.function.description ? `: ${tool.function.description}` : ""}, args: ${JSON.stringify(tool.function.parameters)}`
    ).join("\n");
  }
  return tools.map(
    (tool) => `${tool.name}: ${tool.description}, args: ${JSON.stringify(
      (0, import_zod_to_json_schema.zodToJsonSchema)(
        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        tool.schema
      ).properties
    )}`
  ).join("\n");
}
__name(renderTextDescriptionAndArgs, "renderTextDescriptionAndArgs");

// src/llm-core/agent/react/index.ts
async function createReactAgent({
  llm,
  tools,
  prompt,
  streamRunnable,
  instructions
}) {
  const toolNames = tools.map((tool) => tool.name);
  const toolDescriptions = renderTextDescriptionAndArgs(tools);
  const instructionsFormat = import_prompts2.PromptTemplate.fromTemplate(
    instructions ?? FORMAT_INSTRUCTIONS
  ).format({
    tool_descriptions: toolDescriptions,
    tool_names: toolNames.join(", ")
  });
  prompt = await prompt.partial({
    instructions: /* @__PURE__ */ __name(() => instructionsFormat, "instructions")
  });
  const agent = import_agent2.AgentRunnableSequence.fromRunnables(
    [
      import_runnables4.RunnablePassthrough.assign({
        agent_scratchpad: /* @__PURE__ */ __name((input) => formatLogToString(input.steps), "agent_scratchpad")
      }),
      prompt,
      llm,
      new ReActMultiInputOutputParser({
        toolNames
      })
    ],
    {
      name: "ReactAgent",
      streamRunnable,
      singleAction: false
    }
  );
  return agent;
}
__name(createReactAgent, "createReactAgent");
function formatLogToString(intermediateSteps, observationPrefix = "Observation: ", llmPrefix = "") {
  const formattedSteps = intermediateSteps.reduce(
    (thoughts, { action, observation }) => {
      const actionLog = action.log || `Used tool: ${action.tool}`;
      return thoughts + [actionLog, `
${observationPrefix}${observation}
`].join("\n");
    },
    ""
  );
  return formattedSteps;
}
__name(formatLogToString, "formatLogToString");
// Annotate the CommonJS export names for ESM import in node:
0 && (module.exports = {
  AgentActionOutputParser,
  AgentExecutor,
  AgentExecutorIterator,
  AgentMultiActionOutputParser,
  AgentRunnableSequence,
  BaseAgent,
  BaseMultiActionAgent,
  BaseSingleActionAgent,
  ExceptionTool,
  RunnableMultiActionAgent,
  RunnableSingleActionAgent,
  _formatIntermediateSteps,
  createOpenAIAgent,
  createReactAgent,
  formatLogToString,
  isRunnableAgent
});
