var __defProp = Object.defineProperty;
var __name = (target, value) => __defProp(target, "name", { value, configurable: true });

// src/llm-core/platform/model.ts
import { Embeddings } from "@langchain/core/embeddings";
import {
  BaseChatModel
} from "@langchain/core/language_models/chat_models";
import { sleep } from "koishi";
import {
  getModelContextSize,
  getModelNameForTiktoken,
  messageTypeToOpenAIRole
} from "koishi-plugin-chatluna/llm-core/utils/count_tokens";
import {
  ChatLunaError,
  ChatLunaErrorCode
} from "koishi-plugin-chatluna/utils/error";
import { runAsync, withResolver } from "koishi-plugin-chatluna/utils/promise";

// src/llm-core/utils/chunk.ts
var chunkArray = /* @__PURE__ */ __name((arr, chunkSize) => arr.reduce((chunks, elem, index) => {
  const chunkIndex = Math.floor(index / chunkSize);
  const chunk = chunks[chunkIndex] || [];
  chunks[chunkIndex] = chunk.concat([elem]);
  return chunks;
}, []), "chunkArray");

// src/llm-core/utils/tiktoken.ts
import path from "path";
import {
  getEncodingNameForModel,
  Tiktoken
} from "js-tiktoken/lite";
import {
  chatLunaFetch,
  globalProxyAddress
} from "koishi-plugin-chatluna/utils/request";
import os from "os";
import fs from "fs/promises";
var cache = {};
async function getEncoding(encoding, options) {
  options = options ?? {};
  const cacheDir = path.resolve(os.tmpdir(), "chatluna", "tiktoken");
  const cachePath = path.join(cacheDir, `${encoding}.json`);
  if (cache[encoding]) {
    return new Tiktoken(cache[encoding], options?.extendedSpecialTokens);
  }
  await fs.mkdir(cacheDir, { recursive: true });
  try {
    const cacheContent = await fs.readFile(cachePath, "utf-8");
    cache[encoding] = JSON.parse(cacheContent);
    return new Tiktoken(cache[encoding], options?.extendedSpecialTokens);
  } catch (e) {
  }
  const url = globalProxyAddress.length > 0 ? `https://tiktoken.pages.dev/js/${encoding}.json` : `https://jsd.onmicrosoft.cn/npm/tiktoken@latest/encoders/${encoding}.json`;
  cache[encoding] = await chatLunaFetch(url).then((res) => res.json()).catch((e) => {
    delete cache[encoding];
    throw e;
  });
  await fs.writeFile(cachePath, JSON.stringify(cache[encoding]));
  return new Tiktoken(cache[encoding], options?.extendedSpecialTokens);
}
__name(getEncoding, "getEncoding");
async function encodingForModel(model, options) {
  const result = await getEncoding(getEncodingNameForModel(model), options);
  return result;
}
__name(encodingForModel, "encodingForModel");

// src/llm-core/utils/function_def.ts
import { zodToJsonSchema } from "zod-to-json-schema";
function isAnyOfProp(prop) {
  return prop.anyOf !== void 0 && Array.isArray(prop.anyOf);
}
__name(isAnyOfProp, "isAnyOfProp");
function formatFunctionDefinitions(functions) {
  const lines = ["namespace functions {", ""];
  for (const f of functions) {
    if (f.description) {
      lines.push(`// ${f.description}`);
    }
    const schema = zodToJsonSchema(f.schema);
    if (Object.keys(schema ?? {}).length > 0) {
      lines.push(`type ${f.name} = (_: {`);
      lines.push(formatObjectProperties(schema, 0));
      lines.push("}) => any;");
    } else {
      lines.push(`type ${f.name} = () => any;`);
    }
    lines.push("");
  }
  lines.push("} // namespace functions");
  return lines.join("\n");
}
__name(formatFunctionDefinitions, "formatFunctionDefinitions");
function formatObjectProperties(obj, indent) {
  const lines = [];
  for (const [name, param] of Object.entries(obj.properties ?? {})) {
    if (param.description && indent < 2) {
      lines.push(`// ${param.description}`);
    }
    if (obj.required?.includes(name)) {
      lines.push(`${name}: ${formatType(param, indent)},`);
    } else {
      lines.push(`${name}?: ${formatType(param, indent)},`);
    }
  }
  return lines.map((line) => " ".repeat(indent) + line).join("\n");
}
__name(formatObjectProperties, "formatObjectProperties");
function formatType(param, indent) {
  if (isAnyOfProp(param)) {
    return param.anyOf.map((v) => formatType(v, indent)).join(" | ");
  }
  switch (param.type) {
    case "string":
      if (param.enum) {
        return param.enum.map((v) => `"${v}"`).join(" | ");
      }
      return "string";
    case "number":
      if (param.enum) {
        return param.enum.map((v) => `${v}`).join(" | ");
      }
      return "number";
    case "integer":
      if (param.enum) {
        return param.enum.map((v) => `${v}`).join(" | ");
      }
      return "number";
    case "boolean":
      return "boolean";
    case "null":
      return "null";
    case "object":
      return ["{", formatObjectProperties(param, indent + 2), "}"].join(
        "\n"
      );
    case "array":
      if (param.items) {
        return `${formatType(param.items, indent)}[]`;
      }
      return "any[]";
    default:
      return "";
  }
}
__name(formatType, "formatType");

// src/llm-core/platform/model.ts
var ChatLunaChatModel = class extends BaseChatModel {
  constructor(_options) {
    super(_options);
    this._options = _options;
    this._requester = _options.requester;
    this._modelName = _options.model ?? _options.modelInfo.name;
    this._maxModelContextSize = _options.modelMaxContextSize;
    this._modelInfo = _options.modelInfo;
    this._isThinkModel = _options.isThinkModel ?? false;
  }
  static {
    __name(this, "ChatLunaChatModel");
  }
  // eslint-disable-next-line @typescript-eslint/naming-convention
  __encoding;
  _requester;
  _modelName;
  _maxModelContextSize;
  _modelInfo;
  _isThinkModel;
  // eslint-disable-next-line @typescript-eslint/naming-convention
  lc_serializable = false;
  get callKeys() {
    return [
      ...super.callKeys,
      "model",
      "temperature",
      "maxTokens",
      "maxTokenLimit",
      "topP",
      "frequencyPenalty",
      "presencePenalty",
      "n",
      "logitBias",
      "id",
      "stream",
      "tools"
    ];
  }
  /**
   * Get the parameters used to invoke the model
   */
  invocationParams(options) {
    let maxTokenLimit = options?.maxTokenLimit ?? this._options.maxTokenLimit;
    if (maxTokenLimit < 0 || maxTokenLimit === 0) {
      maxTokenLimit = this._maxModelContextSize / 2;
    }
    const modelName = options?.model ?? this._modelName;
    if (maxTokenLimit != null && maxTokenLimit >= this.getModelMaxContextSize()) {
      maxTokenLimit = this.getModelMaxContextSize();
    }
    return {
      model: modelName,
      temperature: options?.temperature ?? this._options.temperature,
      topP: options?.topP ?? this._options.topP,
      frequencyPenalty: options?.frequencyPenalty ?? this._options.frequencyPenalty,
      presencePenalty: options?.presencePenalty ?? this._options.presencePenalty,
      n: options?.n ?? this._options.n,
      logitBias: options?.logitBias ?? this._options.logitBias,
      maxTokens: options?.maxTokens ?? this._options.maxTokens,
      maxTokenLimit,
      stop: options?.stop ?? this._options.stop,
      stream: options?.stream ?? this._options.stream,
      tools: options?.tools ?? this._options.tools,
      id: options?.id ?? this._options.id,
      signal: options?.signal ?? this._options.signal,
      timeout: options?.timeout ?? this._options.timeout
    };
  }
  async *_streamResponseChunks(messages, options, runManager) {
    const withTool = (options.tools?.length ?? 0) > 0;
    let promptTokens;
    if (withTool) {
      ;
      [messages, promptTokens] = await this.cropMessages(
        messages,
        options["tools"]
      );
    }
    const stream = await this._createStreamWithRetry({
      ...this.invocationParams(options),
      input: messages
    });
    const chunks = [];
    for await (const chunk of stream) {
      yield chunk;
      const chunkText = chunk.text ?? "";
      if (chunkText != null) {
        void runManager?.handleLLMNewToken(chunkText);
      }
      if (withTool) {
        chunks.push(chunk);
      }
    }
    if (withTool && chunks.length > 0) {
      let chunk;
      for (const subChunk of chunks) {
        chunk = chunk ?? subChunk;
        if (chunk !== subChunk) {
          chunk = chunk?.concat(subChunk);
        }
      }
      const completionTokens = await this._countMessageTokens(
        chunk.message
      );
      await runManager?.handleLLMEnd({
        generations: [],
        llmOutput: {
          tokenUsage: {
            completionTokens,
            promptTokens,
            totalTokens: completionTokens + promptTokens
          }
        }
      });
    }
  }
  async _generate(messages, options, runManager) {
    let promptTokens;
    [messages, promptTokens] = await this.cropMessages(
      messages,
      options["tools"]
    );
    const response = await this._generateWithRetry(
      messages,
      options,
      runManager
    );
    if (response == null) {
      throw new ChatLunaError(ChatLunaErrorCode.API_REQUEST_FAILED);
    }
    response.generationInfo = response.generationInfo ?? {};
    if (response.generationInfo.tokenUsage == null) {
      const completionTokens = await this._countMessageTokens(
        response.message
      );
      response.generationInfo.tokenUsage = {
        completionTokens,
        promptTokens,
        totalTokens: completionTokens + promptTokens
      };
    }
    return {
      generations: [response],
      llmOutput: response.generationInfo
    };
  }
  _generateWithRetry(messages, options, runManager) {
    const generateWithRetry = /* @__PURE__ */ __name(async () => {
      let response;
      if (options.stream) {
        const stream = this._streamResponseChunks(
          messages,
          options,
          runManager
        );
        let responseChunk;
        for await (const chunk of stream) {
          responseChunk = responseChunk != null ? responseChunk.concat(chunk) : chunk;
        }
        response = responseChunk;
      } else {
        response = await this._completion({
          ...this.invocationParams(options),
          input: messages
        });
      }
      if (response == null) {
        throw new ChatLunaError(ChatLunaErrorCode.API_REQUEST_FAILED);
      }
      return response;
    }, "generateWithRetry");
    return this.caller.call(generateWithRetry);
  }
  async _withTimeout(func, timeout) {
    const { promise, resolve, reject } = withResolver();
    const timeoutId = setTimeout(() => {
      reject(new ChatLunaError(ChatLunaErrorCode.API_REQUEST_TIMEOUT));
    }, timeout);
    runAsync(async () => {
      let result;
      try {
        result = await func();
        clearTimeout(timeoutId);
      } catch (error) {
        clearTimeout(timeoutId);
        reject(error);
        return;
      }
      clearTimeout(timeoutId);
      resolve(result);
    });
    return promise;
  }
  /**
   ** Creates a streaming request with retry.
   * @param request The parameters for creating a completion.
   ** @returns A streaming request.
   */
  _createStreamWithRetry(params) {
    const makeCompletionRequest = /* @__PURE__ */ __name(async () => {
      try {
        const result = await this._withTimeout(
          async () => this._requester.completionStream(params),
          params.timeout
        );
        return result;
      } catch (e) {
        await sleep(2e3);
        throw e;
      }
    }, "makeCompletionRequest");
    return this.caller.call(makeCompletionRequest);
  }
  /** @ignore */
  async _completion(params) {
    try {
      const result = await this._withTimeout(
        () => this._requester.completion(params),
        params.timeout
      );
      return result;
    } catch (e) {
      await sleep(2e3);
      throw e;
    }
  }
  async cropMessages(messages, tools, systemMessageLength = 1) {
    messages = messages.concat([]);
    const result = [];
    const maxTokenLimit = this.invocationParams().maxTokenLimit;
    let totalTokens = 0;
    if (tools) {
      const promptDefinitions = formatFunctionDefinitions(tools);
      totalTokens += await this.getNumTokens(promptDefinitions);
      totalTokens += 9;
    }
    if (tools && messages.find((m) => m.getType() === "system")) {
      totalTokens -= 4;
    }
    const systemMessages = [];
    let index = 0;
    if (messages.length < systemMessageLength) {
      throw new ChatLunaError(
        ChatLunaErrorCode.UNKNOWN_ERROR,
        new Error("Message length is less than system message length")
      );
    }
    while (index < systemMessageLength) {
      const message = messages.shift();
      systemMessages.push(message);
      totalTokens += await this._countMessageTokens(message);
      index++;
    }
    for (const message of messages.reverse()) {
      const messageTokens = await this._countMessageTokens(message);
      if (totalTokens + messageTokens > maxTokenLimit) {
        break;
      }
      totalTokens += messageTokens;
      result.unshift(message);
    }
    for (const message of systemMessages.reverse()) {
      result.unshift(message);
    }
    return [result, totalTokens];
  }
  async _countMessageTokens(message) {
    let totalCount = 0;
    let tokensPerMessage = 0;
    let tokensPerName = 0;
    if (this.modelName === "gpt-3.5-turbo-0301") {
      tokensPerMessage = 4;
      tokensPerName = -1;
    } else {
      tokensPerMessage = 3;
      tokensPerName = 1;
    }
    const textCount = await this.getNumTokens(
      message?.content ?? ""
    );
    const roleCount = await this.getNumTokens(
      messageTypeToOpenAIRole(message.getType())
    );
    const nameCount = message.name !== void 0 ? tokensPerName + await this.getNumTokens(message.name) : 0;
    let count = textCount + tokensPerMessage + roleCount + nameCount;
    const openAIMessage = message;
    if (openAIMessage.getType() === "function") {
      count -= 2;
    }
    if (openAIMessage.additional_kwargs?.function_call) {
      count += 3;
    }
    if (openAIMessage?.additional_kwargs.function_call?.name) {
      count += await this.getNumTokens(
        openAIMessage.additional_kwargs.function_call?.name
      );
    }
    if (openAIMessage.additional_kwargs.function_call?.arguments && typeof openAIMessage.additional_kwargs.function_call.arguments === "string") {
      count += await this.getNumTokens(
        // Remove newlines and spaces
        JSON.stringify(
          JSON.parse(
            openAIMessage.additional_kwargs.function_call?.arguments
          )
        )
      );
    }
    totalCount += count;
    totalCount += 3;
    return totalCount;
  }
  async clearContext(id) {
    await this._requester.dispose(this.modelName, id);
  }
  getModelMaxContextSize() {
    if (this._maxModelContextSize != null) {
      return this._maxModelContextSize;
    }
    const modelName = this._modelName ?? "gpt2";
    return getModelContextSize(modelName);
  }
  async getNumTokens(text) {
    let numTokens = Math.ceil(text.length / 4);
    if (!this.__encoding) {
      try {
        this.__encoding = await encodingForModel(
          "modelName" in this ? getModelNameForTiktoken(this.modelName) : "gpt2"
        );
      } catch (error) {
      }
    }
    if (this.__encoding) {
      numTokens = this.__encoding.encode(text)?.length ?? numTokens;
    }
    return numTokens;
  }
  _llmType() {
    return this._options?.llmType ?? "openai";
  }
  get modelName() {
    return this._modelName;
  }
  get modelInfo() {
    return this._modelInfo;
  }
  get isThinkModel() {
    return this._isThinkModel;
  }
  _modelType() {
    return "base_chat_model";
  }
  /** @ignore */
  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  _combineLLMOutput(...llmOutputs) {
  }
};
var ChatHubBaseEmbeddings = class extends Embeddings {
  static {
    __name(this, "ChatHubBaseEmbeddings");
  }
};
var ChatLunaEmbeddings = class extends ChatHubBaseEmbeddings {
  static {
    __name(this, "ChatLunaEmbeddings");
  }
  modelName = "text-embedding-ada-002";
  batchSize = 30;
  stripNewLines = true;
  timeout;
  _client;
  constructor(fields) {
    super(fields);
    this.batchSize = fields?.batchSize ?? this.batchSize;
    this.stripNewLines = fields?.stripNewLines ?? this.stripNewLines;
    this.timeout = fields?.timeout ?? 1e3 * 60;
    this.modelName = fields?.model ?? this.modelName;
    this._client = fields?.client;
  }
  async embedDocuments(texts) {
    const subPrompts = chunkArray(
      this.stripNewLines ? texts.map((t) => t.replaceAll("\n", " ")) : texts,
      this.batchSize
    );
    const embeddings = [];
    for (let i = 0; i < subPrompts.length; i += 1) {
      const input = subPrompts[i];
      const data = await this._embeddingWithRetry({
        model: this.modelName,
        input
      });
      for (let j = 0; j < input.length; j += 1) {
        embeddings.push(data[j]);
      }
    }
    return embeddings;
  }
  async embedQuery(text) {
    const data = await this._embeddingWithRetry({
      model: this.modelName,
      input: this.stripNewLines ? text.replaceAll("\n", " ") : text
    });
    if (data[0] instanceof Array) {
      return data[0];
    }
    return data;
  }
  async _embeddingWithRetry(request) {
    request.timeout = request.timeout ?? this.timeout;
    try {
      return await this.caller.call(
        (request2) => {
          return Promise.race([
            new Promise((resolve, reject) => {
              setTimeout(() => {
                reject(
                  Error(
                    `timeout when calling ${this.modelName} embeddings`
                  )
                );
              }, request2.timeout);
            }),
            new Promise(
              // eslint-disable-next-line no-async-promise-executor
              async (resolve, reject) => {
                let data;
                try {
                  data = await this._client.embeddings(request2);
                } catch (e) {
                  if (e instanceof ChatLunaError) {
                    reject(e);
                  } else {
                    reject(
                      new ChatLunaError(
                        ChatLunaErrorCode.API_REQUEST_FAILED,
                        e
                      )
                    );
                  }
                }
                if (data) {
                  resolve(data);
                }
                reject(
                  Error(
                    `error when calling ${this.modelName} embeddings, Result: ` + JSON.stringify(data)
                  )
                );
              }
            )
          ]);
        },
        request
      );
    } catch (e) {
      if (e instanceof ChatLunaError) {
        throw new ChatLunaError(ChatLunaErrorCode.API_REQUEST_FAILED, e);
      } else {
        throw new ChatLunaError(ChatLunaErrorCode.API_REQUEST_FAILED, e);
      }
    }
  }
};
export {
  ChatHubBaseEmbeddings,
  ChatLunaChatModel,
  ChatLunaEmbeddings
};
